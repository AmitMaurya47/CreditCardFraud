{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "square-damages",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in c:\\users\\amit maurya\\anaconda3\\envs\\tensorflow\\lib\\site-packages (1.5.12)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\amit maurya\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied: requests in c:\\users\\amit maurya\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from kaggle) (2.25.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\amit maurya\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from kaggle) (2020.12.5)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\amit maurya\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from kaggle) (1.26.3)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\amit maurya\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\amit maurya\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\amit maurya\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from kaggle) (4.56.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\amit maurya\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\amit maurya\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->kaggle) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\amit maurya\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests->kaggle) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "blond-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported Libraries\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Classifier Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import collections\n",
    "\n",
    "\n",
    "# Other Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "several-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "impressed-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "api.dataset_download_files('mlg-ulb/creditcardfraud')\n",
    "zf = ZipFile('creditcardfraud.zip')\n",
    "#extracted data is saved in the same directory as notebook\n",
    "zf.extractall() \n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "tamil-hands",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= pd.read_csv('creditcard.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "acquired-account",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.168375e-15</td>\n",
       "      <td>3.416908e-16</td>\n",
       "      <td>-1.379537e-15</td>\n",
       "      <td>2.074095e-15</td>\n",
       "      <td>9.604066e-16</td>\n",
       "      <td>1.487313e-15</td>\n",
       "      <td>-5.556467e-16</td>\n",
       "      <td>1.213481e-16</td>\n",
       "      <td>-2.406331e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.654067e-16</td>\n",
       "      <td>-3.568593e-16</td>\n",
       "      <td>2.578648e-16</td>\n",
       "      <td>4.473266e-15</td>\n",
       "      <td>5.340915e-16</td>\n",
       "      <td>1.683437e-15</td>\n",
       "      <td>-3.660091e-16</td>\n",
       "      <td>-1.227390e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.168375e-15  3.416908e-16 -1.379537e-15  2.074095e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.604066e-16  1.487313e-15 -5.556467e-16  1.213481e-16 -2.406331e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.654067e-16 -3.568593e-16  2.578648e-16  4.473266e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.340915e-16  1.683437e-15 -3.660091e-16 -1.227390e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acoustic-arthur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Good No Null Values!\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "unusual-plate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "standard-matter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.83 % of the dataset\n",
      "Frauds 0.17 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "# The classes are heavily skewed we need to solve this issue later.\n",
    "print('No Frauds', round(data['Class'].value_counts()[0]/len(data) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(data['Class'].value_counts()[1]/len(data) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-embassy",
   "metadata": {},
   "source": [
    "Note: Notice how imbalanced is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis we might get a lot of errors and our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "peripheral-happiness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Class Distributions \\n (0: No Fraud || 1: Fraud)')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot('Class', data=data)\n",
    "plt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-fields",
   "metadata": {},
   "source": [
    "We will first scale the columns comprise of Time and Amount . Time and amount should be scaled as the other columns. On the other hand, we need to also create a sub sample of the dataframe in order to have an equal amount of Fraud and Non-Fraud cases, helping our algorithms better understand patterns that determines whether a transaction is a fraud or not.\n",
    "\n",
    " <font size=\"5\"> Why do we need scaling ? </font>\n",
    "\n",
    "Machine learning algorithm just sees number â€” if there is a vast difference in the range say few ranging in thousands and few ranging in the tens, and it makes the underlying assumption that higher ranging numbers have superiority of some sort. So these more significant number starts playing a more decisive role while training the model.\n",
    "The machine learning algorithm works on numbers and does not know what that number represents. A weight of 10 grams and a price of 10 dollars represents completely two different things â€” which is a no brainer for humans, but for a model as a feature, it treats both as same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "governmental-barcelona",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# RobustScaler is less prone to outliers.\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "rob_scaler = RobustScaler()\n",
    "\n",
    "data['scaled_amount'] = rob_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\n",
    "data['scaled_time'] = rob_scaler.fit_transform(data['Time'].values.reshape(-1,1))\n",
    "\n",
    "data.drop(['Time','Amount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "closed-stack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.783274</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.269825</td>\n",
       "      <td>-0.994983</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.983721</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.418291</td>\n",
       "      <td>-0.994972</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.670579</td>\n",
       "      <td>-0.994960</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   scaled_amount  scaled_time        V1        V2        V3        V4  \\\n",
       "0       1.783274    -0.994983 -1.359807 -0.072781  2.536347  1.378155   \n",
       "1      -0.269825    -0.994983  1.191857  0.266151  0.166480  0.448154   \n",
       "2       4.983721    -0.994972 -1.358354 -1.340163  1.773209  0.379780   \n",
       "3       1.418291    -0.994972 -0.966272 -0.185226  1.792993 -0.863291   \n",
       "4       0.670579    -0.994960 -1.158233  0.877737  1.548718  0.403034   \n",
       "\n",
       "         V5        V6        V7        V8  ...       V20       V21       V22  \\\n",
       "0 -0.338321  0.462388  0.239599  0.098698  ...  0.251412 -0.018307  0.277838   \n",
       "1  0.060018 -0.082361 -0.078803  0.085102  ... -0.069083 -0.225775 -0.638672   \n",
       "2 -0.503198  1.800499  0.791461  0.247676  ...  0.524980  0.247998  0.771679   \n",
       "3 -0.010309  1.247203  0.237609  0.377436  ... -0.208038 -0.108300  0.005274   \n",
       "4 -0.407193  0.095921  0.592941 -0.270533  ...  0.408542 -0.009431  0.798278   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  Class  \n",
       "0 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
       "1  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
       "2  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
       "3 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
       "4 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_amount = data['scaled_amount']\n",
    "scaled_time = data['scaled_time']\n",
    "\n",
    "data.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "data.insert(0, 'scaled_amount', scaled_amount)\n",
    "data.insert(1, 'scaled_time', scaled_time)\n",
    "\n",
    "# Amount and Time are Scaled!\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-madrid",
   "metadata": {},
   "source": [
    "<font size=\"5\"> Splitting the Data (Original DataFrame) </font>\n",
    "\n",
    "Before proceeding with the Random UnderSampling technique we have to separate the orginal dataframe. Why? for testing purposes, remember although we are splitting the data when implementing Random UnderSampling or OverSampling techniques, we want to test our models on the original testing set not on the testing set created by either of these techniques. The main goal is to fit the model either with the dataframes that were undersample and oversample (in order for our models to detect the patterns), and test it on the original testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-annual",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "contrary-shaft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds 99.83 % of the dataset\n",
      "Frauds 0.17 % of the dataset\n",
      "Train: [ 30473  30496  31002 ... 284804 284805 284806] Test: [    0     1     2 ... 57017 57018 57019]\n",
      "Train: [     0      1      2 ... 284804 284805 284806] Test: [ 30473  30496  31002 ... 113964 113965 113966]\n",
      "Train: [     0      1      2 ... 284804 284805 284806] Test: [ 81609  82400  83053 ... 170946 170947 170948]\n",
      "Train: [     0      1      2 ... 284804 284805 284806] Test: [150654 150660 150661 ... 227866 227867 227868]\n",
      "Train: [     0      1      2 ... 227866 227867 227868] Test: [212516 212644 213092 ... 284804 284805 284806]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Label Distributions: \n",
      "\n",
      "[0.99827076 0.00172924]\n",
      "[0.99827952 0.00172048]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "print('No Frauds', round(data['Class'].value_counts()[0]/len(data) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(data['Class'].value_counts()[1]/len(data) * 100,2), '% of the dataset')\n",
    "\n",
    "X = data.drop('Class', axis=1)\n",
    "y = data['Class']\n",
    "\n",
    "sss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n",
    "    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n",
    "# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the Distribution of the labels\n",
    "\n",
    "\n",
    "# Turn into an array\n",
    "original_Xtrain = original_Xtrain.values\n",
    "original_Xtest = original_Xtest.values\n",
    "original_ytrain = original_ytrain.values\n",
    "original_ytest = original_ytest.values\n",
    "\n",
    "# See if both the train and test label distribution are similarly distributed\n",
    "train_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\n",
    "test_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\n",
    "print('-' * 100)\n",
    "\n",
    "print('Label Distributions: \\n')\n",
    "print(train_counts_label/ len(original_ytrain))\n",
    "print(test_counts_label/ len(original_ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-revision",
   "metadata": {},
   "source": [
    "<font size=\"5\">What is a sub-Sample?</font>\n",
    "\n",
    "In this scenario, our subsample will be a dataframe with a 50/50 ratio of fraud and non-fraud transactions. Meaning our sub-sample will have the same amount of fraud and non fraud transactions.\n",
    "\n",
    "<font size=\"3\">Why do we create a sub-Sample?</font>\n",
    "\n",
    "In the beginning of this notebook we saw that the original dataframe was heavily imbalanced! Using the original dataframe will cause the following issues:\n",
    "\n",
    "-  Overfitting: Our classification models will assume that in most cases there are no frauds! What we want for our model is to be certain when a fraud occurs.\n",
    "\n",
    "-  Wrong Correlations: Although we don't know what the \"V\" features stand for, it will be useful to understand how each of this features influence the result (Fraud or No Fraud) by having an imbalance dataframe we are not able to see the true correlations between the class and features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-leonard",
   "metadata": {},
   "source": [
    "Steps:\n",
    "- The first thing we have to do is determine how imbalanced is our class (use \"value_counts()\" on the class column to determine the amount for each label)\n",
    "- Once we determine how many instances are considered fraud transactions (Fraud = \"1\") , we should bring the non-fraud transactions to the same amount as fraud transactions (assuming we want a 50/50 ratio), this will be equivalent to 492 cases of fraud and 492 cases of non-fraud transactions.\n",
    "- After implementing this technique, we have a sub-sample of our dataframe with a 50/50 ratio with regards to our classes. Then the next step we will implement is to shuffle the data to see if our models can maintain a certain accuracy everytime we run this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "floral-marker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_amount</th>\n",
       "      <th>scaled_time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116685</th>\n",
       "      <td>-0.279746</td>\n",
       "      <td>-0.121042</td>\n",
       "      <td>-1.063187</td>\n",
       "      <td>1.869203</td>\n",
       "      <td>0.441113</td>\n",
       "      <td>0.426135</td>\n",
       "      <td>-1.076166</td>\n",
       "      <td>0.432954</td>\n",
       "      <td>-3.161861</td>\n",
       "      <td>-7.955931</td>\n",
       "      <td>...</td>\n",
       "      <td>1.846824</td>\n",
       "      <td>-4.018247</td>\n",
       "      <td>0.396339</td>\n",
       "      <td>0.580662</td>\n",
       "      <td>0.647825</td>\n",
       "      <td>0.530677</td>\n",
       "      <td>0.095528</td>\n",
       "      <td>-0.097458</td>\n",
       "      <td>0.240792</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9035</th>\n",
       "      <td>-0.293440</td>\n",
       "      <td>-0.846991</td>\n",
       "      <td>-2.589617</td>\n",
       "      <td>7.016714</td>\n",
       "      <td>-13.705407</td>\n",
       "      <td>10.343228</td>\n",
       "      <td>-2.954461</td>\n",
       "      <td>-3.055116</td>\n",
       "      <td>-9.301289</td>\n",
       "      <td>3.349573</td>\n",
       "      <td>...</td>\n",
       "      <td>1.488855</td>\n",
       "      <td>1.887738</td>\n",
       "      <td>0.333998</td>\n",
       "      <td>0.287659</td>\n",
       "      <td>-1.186406</td>\n",
       "      <td>-0.690273</td>\n",
       "      <td>0.631704</td>\n",
       "      <td>1.934221</td>\n",
       "      <td>0.789687</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276904</th>\n",
       "      <td>-0.237686</td>\n",
       "      <td>0.971111</td>\n",
       "      <td>1.906657</td>\n",
       "      <td>-0.097042</td>\n",
       "      <td>-1.909916</td>\n",
       "      <td>0.076906</td>\n",
       "      <td>1.183669</td>\n",
       "      <td>1.128726</td>\n",
       "      <td>-0.199456</td>\n",
       "      <td>0.401240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.257904</td>\n",
       "      <td>-0.245696</td>\n",
       "      <td>-0.551535</td>\n",
       "      <td>0.388398</td>\n",
       "      <td>-0.834519</td>\n",
       "      <td>-0.499669</td>\n",
       "      <td>0.269871</td>\n",
       "      <td>-0.016787</td>\n",
       "      <td>-0.055015</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12070</th>\n",
       "      <td>-0.293440</td>\n",
       "      <td>-0.749081</td>\n",
       "      <td>-16.367923</td>\n",
       "      <td>9.223692</td>\n",
       "      <td>-23.270631</td>\n",
       "      <td>11.844777</td>\n",
       "      <td>-9.462037</td>\n",
       "      <td>-2.450444</td>\n",
       "      <td>-16.925152</td>\n",
       "      <td>1.384208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993585</td>\n",
       "      <td>-2.343674</td>\n",
       "      <td>1.004602</td>\n",
       "      <td>1.188212</td>\n",
       "      <td>-1.047184</td>\n",
       "      <td>-0.035573</td>\n",
       "      <td>0.664900</td>\n",
       "      <td>2.122796</td>\n",
       "      <td>-1.416741</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135718</th>\n",
       "      <td>-0.201635</td>\n",
       "      <td>-0.039004</td>\n",
       "      <td>-0.885254</td>\n",
       "      <td>1.790649</td>\n",
       "      <td>-0.945149</td>\n",
       "      <td>3.853433</td>\n",
       "      <td>-1.543510</td>\n",
       "      <td>0.188582</td>\n",
       "      <td>-2.988383</td>\n",
       "      <td>1.344059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370041</td>\n",
       "      <td>0.746160</td>\n",
       "      <td>0.550802</td>\n",
       "      <td>-0.034882</td>\n",
       "      <td>-0.567608</td>\n",
       "      <td>-0.528318</td>\n",
       "      <td>0.258782</td>\n",
       "      <td>0.506893</td>\n",
       "      <td>0.176736</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        scaled_amount  scaled_time         V1        V2         V3         V4  \\\n",
       "116685      -0.279746    -0.121042  -1.063187  1.869203   0.441113   0.426135   \n",
       "9035        -0.293440    -0.846991  -2.589617  7.016714 -13.705407  10.343228   \n",
       "276904      -0.237686     0.971111   1.906657 -0.097042  -1.909916   0.076906   \n",
       "12070       -0.293440    -0.749081 -16.367923  9.223692 -23.270631  11.844777   \n",
       "135718      -0.201635    -0.039004  -0.885254  1.790649  -0.945149   3.853433   \n",
       "\n",
       "              V5        V6         V7        V8  ...       V20       V21  \\\n",
       "116685 -1.076166  0.432954  -3.161861 -7.955931  ...  1.846824 -4.018247   \n",
       "9035   -2.954461 -3.055116  -9.301289  3.349573  ...  1.488855  1.887738   \n",
       "276904  1.183669  1.128726  -0.199456  0.401240  ... -0.257904 -0.245696   \n",
       "12070  -9.462037 -2.450444 -16.925152  1.384208  ...  0.993585 -2.343674   \n",
       "135718 -1.543510  0.188582  -2.988383  1.344059  ...  0.370041  0.746160   \n",
       "\n",
       "             V22       V23       V24       V25       V26       V27       V28  \\\n",
       "116685  0.396339  0.580662  0.647825  0.530677  0.095528 -0.097458  0.240792   \n",
       "9035    0.333998  0.287659 -1.186406 -0.690273  0.631704  1.934221  0.789687   \n",
       "276904 -0.551535  0.388398 -0.834519 -0.499669  0.269871 -0.016787 -0.055015   \n",
       "12070   1.004602  1.188212 -1.047184 -0.035573  0.664900  2.122796 -1.416741   \n",
       "135718  0.550802 -0.034882 -0.567608 -0.528318  0.258782  0.506893  0.176736   \n",
       "\n",
       "        Class  \n",
       "116685      0  \n",
       "9035        1  \n",
       "276904      0  \n",
       "12070       1  \n",
       "135718      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n",
    "\n",
    "# Lets shuffle the data before creating the subsamples\n",
    "\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "# amount of fraud classes 492 rows.\n",
    "fraud_data = data.loc[data['Class'] == 1]\n",
    "non_fraud_data = data.loc[data['Class'] == 0][:492]\n",
    "\n",
    "normal_distributed_data = pd.concat([fraud_data, non_fraud_data])\n",
    "\n",
    "# Shuffle dataframe rows\n",
    "new_data = normal_distributed_data.sample(frac=1, random_state=42)\n",
    "\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "perceived-wrapping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of the Classes in the subsample dataset\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: Class, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Distribution of the Classes in the subsample dataset')\n",
    "print(new_data['Class'].value_counts()/len(new_data))\n",
    "\n",
    "\n",
    "\n",
    "sns.countplot('Class', data=new_data)\n",
    "plt.title('Equally Distributed Classes', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "boring-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling \n",
    "X = new_data.drop('Class', axis=1)\n",
    "y = new_data['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-symbol",
   "metadata": {},
   "source": [
    "<font size=\"5\">Classifiers (UnderSampling):</font>\n",
    "\n",
    "In this section we will train four types of classifiers and decide which classifier will be more effective in detecting fraud transactions. Before we have to split our data into training and testing sets and separate the features from the labels.\n",
    "Summary:\n",
    "- Logistic Regression classifier is more accurate than the other three classifiers in most cases. (We will further analyze Logistic Regression)\n",
    "- Logistic Regression has the best Receiving Operating Characteristic score (ROC), meaning that LogisticRegression pretty accurately separates fraud and non-fraud transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "nearby-perry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data is already scaled we should split our training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This is explicitly used for undersampling.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "greater-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the values into an array for feeding the classification algorithms.\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "physical-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement simple classifiers\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "wired-equation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifiers:  LogisticRegression Has a training score of 94.0 % accuracy score\n",
      "Classifiers:  KNeighborsClassifier Has a training score of 92.0 % accuracy score\n",
      "Classifiers:  SVC Has a training score of 93.0 % accuracy score\n",
      "Classifiers:  DecisionTreeClassifier Has a training score of 90.0 % accuracy score\n"
     ]
    }
   ],
   "source": [
    "# Wow our scores are getting even high scores even when applying cross validation.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "million-monday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifiers:  LogisticRegression Has a test score of 94.0 % accuracy score\n",
      "Classifiers:  KNeighborsClassifier Has a test score of 93.0 % accuracy score\n",
      "Classifiers:  SVC Has a test score of 93.0 % accuracy score\n",
      "Classifiers:  DecisionTreeClassifier Has a test score of 92.0 % accuracy score\n"
     ]
    }
   ],
   "source": [
    "for key, classifier in classifiers.items():\n",
    "    test_prediction=classifier.predict(X_test)\n",
    "    test_score = accuracy_score(test_prediction,y_test)\n",
    "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a test score of\", round(test_score.mean(),2)*100 , \"% accuracy score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "reasonable-swaziland",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-3bf506238112>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Create a DataFrame with all the scores and the classifiers names.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m log_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5,\n\u001b[0m\u001b[0;32m      6\u001b[0m                              method=\"decision_function\")\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'log_reg' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "# Create a DataFrame with all the scores and the classifiers names.\n",
    "\n",
    "log_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5,\n",
    "                             method=\"decision_function\")\n",
    "\n",
    "knears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n",
    "\n",
    "svc_pred = cross_val_predict(svc, X_train, y_train, cv=5,\n",
    "                             method=\"decision_function\")\n",
    "\n",
    "tree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\n",
    "print('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\n",
    "print('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\n",
    "print('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\n",
    "knear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\n",
    "svc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\n",
    "tree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\n",
    "\n",
    "\n",
    "def graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n",
    "    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n",
    "    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n",
    "    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n",
    "    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([-0.01, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n",
    "                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n",
    "                )\n",
    "    plt.legend()\n",
    "    \n",
    "graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-photography",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
